{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec as wv\n",
    "import gensim\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "import MeCab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "class StoryFeeling:\n",
    "    def __init__(self,text_file_path,model,title):\n",
    "        self.text_file_path = text_file_path\n",
    "        self.model = model\n",
    "        self.title = title\n",
    "    def process_text(self):\n",
    "        with open(self.text_file_path,encoding='utf-8') as input_file:\n",
    "            sentences = input_file.readlines()\n",
    "            sentences = ''.join(sentences)\n",
    "            sentence_array = ''\n",
    "            sentence_list = []\n",
    "            for sentence in sentences:\n",
    "                if sentence == '。':\n",
    "                    sentence_list.append(sentence_array)\n",
    "                    sentence_array = ''\n",
    "                else:\n",
    "                    sentence_array += sentence\n",
    "        sentence_list = list(map(lambda x:x.strip().replace('\\u3000','').replace('\\n',''),sentence_list))\n",
    "        self.sentence_list = sentence_list\n",
    "    def feeling_analyzer(self,feelingword):\n",
    "        tagger = MeCab.Tagger('-Ochasen')\n",
    "        tokenlist = []\n",
    "        tokenlists = []\n",
    "        neg_idx_list = []\n",
    "        for idx ,sentence in enumerate(self.sentence_list):\n",
    "            node = tagger.parseToNode(sentence)\n",
    "            while node:\n",
    "                features = node.feature.split(',')\n",
    "                if features[0] != 'BOS/EOS':\n",
    "                    if features[0] in ['助詞','助動詞'] and features[6] in ['ず','ぬ','ん','ない']:\n",
    "                    #print(features[6])\n",
    "                        neg_idx_list.append(idx)\n",
    "                    if features[0] not in ['助詞','助動詞','記号']:\n",
    "                        token = features[6] if features[6] != '*' else node.surface#*なら見出し語がトークン\n",
    "                        tokenlist.append(token)\n",
    "                node = node.next\n",
    "            tokenlists.append(tokenlist)\n",
    "            tokenlist = []\n",
    "        vec_avg_list = []\n",
    "        strong_flag = 0\n",
    "        weak_flag = 0\n",
    "        for sen_num ,tokens in enumerate(tokenlists):\n",
    "            vec_sum = 0\n",
    "            for word in tokens:\n",
    "                #print(word)\n",
    "                if word in [\"非常\", \"たいへん\",\"極めて\",\"たいそう\",\"かなり\",\"すごく\",\"とても\"]:\n",
    "                        strong_flag += 1\n",
    "                if word in [\"すこし\",\"ちょっと\",\"やや\",\"少々\",\"いくらか\",\"わずかに\"]:\n",
    "                        weak_flag += 1\n",
    "                try:\n",
    "                    simile = self.model.wv.similarity(w1=feelingword, w2=word)\n",
    "                    #print(simile)\n",
    "                    similetofeeling = self.model.wv.similarity(w1=\"感情\", w2=word)\n",
    "                    #print(simile)\n",
    "                    if similetofeeling >= 0.3:\n",
    "                        simile *= 1.5\n",
    "                    else:\n",
    "                        simile *= 0.8\n",
    "                    vec_sum += simile*100\n",
    "                    if strong_flag == 1:\n",
    "                        #print(word)\n",
    "                        vec_sum *= 1.5\n",
    "                        strong_flag = 0\n",
    "                    if weak_flag == 1:\n",
    "                        #print(word)\n",
    "                        vec_sum *= 0.7\n",
    "                        weak_flag = 0\n",
    "                except KeyError:#学習モデルの中に入っていない語彙\n",
    "                    pass\n",
    "                \n",
    "            vec_avg = vec_sum/len(tokens)\n",
    "            vec_avg= round(vec_avg,3)\n",
    "            vec_avg_list.append(vec_avg)\n",
    "            vec_avg = 0\n",
    "        #vec_avg_array = np.array(vec_avg_list).reshape(-1,1)\n",
    "        list_med = np.median(vec_avg_list)\n",
    "        list_med_0=list(map(lambda x:round(x-list_med,3),vec_avg_list)) \n",
    "        dict_mean3 = {}\n",
    "        for neg_idx in neg_idx_list:\n",
    "            list_med_0[neg_idx] = -(list_med_0[neg_idx])\n",
    "        for idx in range(0,len(list_med_0)-2):\n",
    "            mean3 = round(sum(list_med_0[idx:idx+4])/4,4)\n",
    "            dict_mean3[idx] = mean3\n",
    "        sorted_dict_mean3 = sorted(dict_mean3.items(),key=lambda x:x[1],reverse=True)\n",
    "        counth = 0\n",
    "        countl = 0\n",
    "        highsentences_list = []\n",
    "        lowsentences_list =[]\n",
    "        for key , _ in sorted_dict_mean3:\n",
    "            highsentences = sentence_list[key:key+4]\n",
    "            highsentences_list.append(highsentences)\n",
    "            counth += 1\n",
    "            if counth == 3:\n",
    "                break\n",
    "        for key, _ in reversed(sorted_dict_mean3):\n",
    "            lowsentences = sentence_list[key:key+4]\n",
    "            lowsentences_list.append(lowsentences)\n",
    "            countl += 1\n",
    "            if countl == 3:\n",
    "                break\n",
    "    \n",
    "        self.feel_avg_dict = dict_mean3\n",
    "        self.high = highsentences_list\n",
    "        self.low = lowsentences_list\n",
    "        #print(dict_mean3,highsentences_list,lowsentences_list)\n",
    "    def make_graph(self):\n",
    "        fig = plt.figure(figsize=(16,4))\n",
    "        ax = fig.add_subplot(1,1,1,title='The Little Match Girl')\n",
    "        dic_to_list = list(self.feel_avg_dict.values())\n",
    "        ax.plot(dic_to_list)\n",
    "        \n",
    "        ax.set_ylim([min(dic_to_list)-10, max(dic_to_list)+10])\n",
    "        plt.show()\n",
    "        print(\"1st high score:\\n{}。\".format('。'.join(self.high[0])))\n",
    "        print(\"2nd high score:\\n{}。\".format('。'.join(self.high[1])))\n",
    "        print(\"3rd high score:\\n{}。\".format('。'.join(self.high[2])))\n",
    "        print(\"1st low score:\\n{}。\".format('。'.join(self.low[0])))\n",
    "        print(\"2nd low score:\\n{}。\".format('。'.join(self.low[1])))\n",
    "        print(\"3rd low score:\\n{}。\".format('。'.join(self.low[2])))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = wv.load('./latest-ja-word2vec-gensim-model/word2vec.gensim.model')\n",
    "    text_file_path = \"machi.txt\"\n",
    "    title = 'The Little Match Girl'\n",
    "    st = StoryFeeling(text_file_path=text_file_path,model=model,title=title)\n",
    "    st.process_text()\n",
    "    st.feeling_analyzer('うれしい')\n",
    "    st.make_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
